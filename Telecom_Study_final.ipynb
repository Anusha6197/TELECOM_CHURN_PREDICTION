{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7XiAGJNhzWN"
   },
   "source": [
    "# Telecom Churn Case Study\n",
    "\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the number one business goal.\n",
    "To reduce customer churn, telecom companies need to predict which customers are at high risk of churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Objective\n",
    "The business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. Also recommend strategies to manage customer churn based on your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "POdJ4Tu_h_Nt"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8TLl0GaiLde"
   },
   "outputs": [],
   "source": [
    "# pip install fancyimpute\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9cMHtaBciaRP"
   },
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JbxAupehiHtA"
   },
   "outputs": [],
   "source": [
    "telecom_data = pd.read_csv('telecom_churn_data.csv')\n",
    "print('No of rows within the telecom data is -',len(telecom_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxWv0MbJCKqM"
   },
   "source": [
    "## View the Percentage of Missing values withina column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WoziSZNtiwmJ"
   },
   "outputs": [],
   "source": [
    "((telecom_data.isnull().sum()/len(telecom_data))*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qkgSxkujZkZ"
   },
   "source": [
    "## Filter High Valued Customer\n",
    "- In the below defined steps we are calculating the high value customers by using both data usage and recharge amount\n",
    "- So we calculate total recharge for data by multiplying total recharge date and average recharge amount of data\n",
    "- Then we calculate average of both the 6th and 7th month defining 'good' phase\n",
    "- Those peoples who have their average more than 70th percentile value of average of 6 and 7th month are named as <b>High Valued Customer<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYIaB35sjVEr"
   },
   "outputs": [],
   "source": [
    "## Derive Total Data Recharge Amt\n",
    "telecom_data[\"total_rech_data_amt_6\"] = telecom_data[\"total_rech_data_6\"]  * telecom_data['av_rech_amt_data_6']\n",
    "telecom_data[\"total_rech_data_amt_7\"] = telecom_data[\"total_rech_data_7\"]  * telecom_data['av_rech_amt_data_7']\n",
    "telecom_data[\"total_rech_data_amt_8\"] = telecom_data[\"total_rech_data_8\"]  * telecom_data['av_rech_amt_data_8']\n",
    "telecom_data[\"total_rech_data_amt_9\"] = telecom_data[\"total_rech_data_9\"]  * telecom_data['av_rech_amt_data_9']\n",
    "\n",
    "## Drop total_rech_data_* and av_rech_amt_data_*\n",
    "drop_col = [\"total_rech_data_6\", \"total_rech_data_7\", \"total_rech_data_8\", \"total_rech_data_9\", \n",
    "                'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9']\n",
    "telecom_data.drop(drop_col, axis=1, inplace=True)\n",
    "\n",
    "av_rech_amt_6n7 = (telecom_data[\"total_rech_amt_6\"].fillna(0) + telecom_data[\"total_rech_data_amt_6\"].fillna(0) + \n",
    "telecom_data[\"total_rech_amt_7\"].fillna(0) + telecom_data[\"total_rech_data_amt_7\"].fillna(0))/2.0\n",
    "\n",
    "percentile_70 = np.percentile(av_rech_amt_6n7,70)\n",
    "print('The 70 Percentile value for the month of 6 and 7 is -',percentile_70)\n",
    "\n",
    "telecom_data = telecom_data[(av_rech_amt_6n7>=percentile_70)]\n",
    "print('The No. of rows after filtering the data based on 70th Percentile -',len(telecom_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLnVAtdykNvk"
   },
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### Remove Fatures with Single Unique Value\n",
    "\n",
    "- Features with sigle unique values does not impact on churn\n",
    "- So it doesn't make sense to keep them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qndr__IBkIZG"
   },
   "outputs": [],
   "source": [
    "## Remove Data which has only 1 unique Value\n",
    "telecom_data1 = telecom_data.loc[:,telecom_data.apply(pd.Series.nunique) != 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEzOT4I3kWKo"
   },
   "source": [
    "### Convert Column Names to Numeric to keep it Standardised Throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFC9WhkTkSa4"
   },
   "outputs": [],
   "source": [
    "telecom_data1 = telecom_data1.rename(columns={'aug_vbc_3g':'vbc_3g_8','jul_vbc_3g':'vbc_3g_7','jun_vbc_3g':'vbc_3g_6','sep_vbc_3g':'vbc_3g_9'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIIc0Agln_RP"
   },
   "source": [
    "### Check for Object Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEtINkfQk2g5"
   },
   "outputs": [],
   "source": [
    "# Convert the data type of columns in telecom_data1 to Data Time\n",
    "object_data = telecom_data1.select_dtypes(include='object')\n",
    "for col in object_data.columns:\n",
    "    telecom_data1[col] = pd.to_datetime(telecom_data1[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ozerc6B-ohK_"
   },
   "source": [
    "### Drop Columns with > 30 % Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-vsJ-iIk5dG"
   },
   "outputs": [],
   "source": [
    "percentage_missing = pd.DataFrame({'Percent_Missing':telecom_data1.isnull().sum()/len(telecom_data1)})\n",
    "cols_drop = percentage_missing[(percentage_missing['Percent_Missing']>0.3)&(~percentage_missing.index.str.contains('_9'))].index\n",
    "telecom_data2 = telecom_data1.drop(cols_drop,axis=1)\n",
    "telecom_data2.dropna(how='all',axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiAq6b_Ooog_"
   },
   "outputs": [],
   "source": [
    "# Find Columns with Unique Value but Insignificant Frequency\n",
    "# If one the the frequency of one of the values for a column is more than 95%, drop that column\n",
    "for col_name in telecom_data2.columns:\n",
    "    if (len(telecom_data2[col_name].unique()) <= 8):\n",
    "        print(telecom_data2[col_name].value_counts())\n",
    "        print(f\"\\n{35 * '-'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Highly Correlated Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLzcbLU_D2Ye"
   },
   "outputs": [],
   "source": [
    "## Find the correlated variables \n",
    "correlation = telecom_data2.corr()\n",
    "upper = correlation.where(np.triu(np.ones(correlation.shape),k=1).astype(bool))\n",
    "# Drop columns with more than 80 percent correlation with other variables\n",
    "to_drop = [column for column in upper.columns if any(upper[column]>0.80)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehBdyEHyorUU"
   },
   "outputs": [],
   "source": [
    "# Drop columns with high correlations, ['mobile_number'] as well other columns of not significance\n",
    "to_drop = ['mobile_number','loc_og_t2m_mou_6','std_og_t2t_mou_6','std_og_t2t_mou_7','std_og_t2t_mou_8','std_og_t2t_mou_9','std_og_t2m_mou_6',\n",
    "                'std_og_t2m_mou_7','std_og_t2m_mou_8','std_og_t2m_mou_9','total_og_mou_6','total_og_mou_7','total_og_mou_8',\n",
    "                'loc_ic_t2t_mou_6','loc_ic_t2t_mou_7','loc_ic_t2t_mou_8','loc_ic_t2t_mou_9','loc_ic_t2m_mou_6','loc_ic_t2m_mou_7','loc_ic_t2m_mou_8','loc_ic_t2m_mou_9',\n",
    "                'std_ic_t2m_mou_6','std_ic_t2m_mou_7','std_ic_t2m_mou_8','std_ic_t2m_mou_9','total_ic_mou_6','total_ic_mou_7','total_ic_mou_8',\n",
    "                'total_rech_amt_6','total_rech_amt_7','total_rech_amt_8','total_rech_amt_9','arpu_2g_9','count_rech_2g_9','count_rech_3g_9','vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8',\n",
    "                'loc_og_t2t_mou_6','loc_og_t2t_mou_7','loc_og_t2t_mou_8','loc_og_t2t_mou_9','loc_og_t2f_mou_6','loc_og_t2f_mou_7','loc_og_t2f_mou_8','loc_og_t2f_mou_9',\n",
    "                'loc_og_t2m_mou_6','loc_og_t2m_mou_7','loc_og_t2m_mou_8','loc_og_t2m_mou_9','loc_ic_t2f_mou_6','loc_ic_t2f_mou_7','loc_ic_t2f_mou_8','loc_ic_t2f_mou_9'\n",
    "               ,'date_of_last_rech_6','arpu_6','arpu_7','arpu_9','date_of_last_rech_7','date_of_last_rech_8','date_of_last_rech_9','date_of_last_rech_data_9','night_pck_user_9','arpu_3g_9','max_rech_data_9','total_rech_data_amt_9','fb_user_9']\n",
    "\n",
    "telecom_data3 = telecom_data2.drop(to_drop,axis=1)\n",
    "\n",
    "telecom_data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfWWGHAWotrf"
   },
   "outputs": [],
   "source": [
    "#telecom_data3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7izXha1_jeL"
   },
   "source": [
    "## Tag Churn\n",
    "- The churned customers are the ones who hav not used any voice or data in the 9th month\n",
    "- If a customer is churned, the customer will be tagged as 1. Else 0\n",
    "- We have don't a mistake of not considering data consumption for 9th month and also reversly tagged the churning ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JnJgAtyB_gCT"
   },
   "outputs": [],
   "source": [
    "#telecom_data3['churn'] = np.where(telecom_data3[['vol_2g_mb_9','vol_3g_mb_9','count_rech_2g_9', 'count_rech_3g_9']].sum(axis=1)==0,0,1)\n",
    "telecom_data3['churn'] = np.where(telecom_data3[['vol_2g_mb_9','vol_3g_mb_9']].sum(axis=1)==0,0,1)\n",
    "#Remove All 9th Month related columns\n",
    "drop_cols = [col for col in telecom_data3.columns if '_9' in col]\n",
    "print(drop_cols)\n",
    "\n",
    "telecom_data3.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "telecom_data3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqAaU3_toxuf"
   },
   "source": [
    "## Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmJXpFAYITBI"
   },
   "outputs": [],
   "source": [
    "# # Remove date columns (starts with: date)\n",
    "# telecom_data3.drop([col for col in telecom_data3.columns if 'date_' in col], axis=1, inplace=True)\n",
    "# telecom_data3 = telecom_data3.drop(telecom_data3.columns[telecom_data3.dtypes=='datetime64[ns]'],axis=1)\n",
    "telecom_data3 = telecom_data3.drop(telecom_data3.columns[telecom_data3.dtypes=='<M8[ns]'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values using IterativeImputer from fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsWWCfCdo2LS"
   },
   "outputs": [],
   "source": [
    "telecom_data3_columns = telecom_data3.columns\n",
    "ii = IterativeImputer()\n",
    "df_clean = pd.DataFrame(ii.fit_transform(telecom_data3))\n",
    "df_clean.columns = telecom_data3_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wEcuikJApCmz"
   },
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t__yZM9FXUv"
   },
   "outputs": [],
   "source": [
    "# Distribution graphs (histogram/bar graph) of column data\n",
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
    "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "    for i in range(min(nCol, nGraphShown)):\n",
    "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "            valueCounts = columnDf.value_counts()\n",
    "            valueCounts.plot.bar()\n",
    "        else:\n",
    "            columnDf.hist()\n",
    "        plt.ylabel('counts')\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.title(f'{columnNames[i]} (column {i})')\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()\n",
    "  \n",
    "# Correlation matrix\n",
    "def plotCorrelationMatrix(df, graphWidth):\n",
    "    filename = \"Telecom Churn\"\n",
    "    df = df.dropna('columns') # drop columns with NaN\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    if df.shape[1] < 2:\n",
    "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
    "        return\n",
    "    corr = df.corr()\n",
    "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
    "    corrMat = plt.matshow(corr, fignum = 1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.colorbar(corrMat)\n",
    "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
    "    plt.show()\n",
    "# Scatter and density plots\n",
    "def plotScatterMatrix(df, plotSize, textSize):\n",
    "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
    "    # Remove rows and columns that would lead to df being singular\n",
    "    df = df.dropna('columns')\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    columnNames = list(df)\n",
    "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
    "        columnNames = columnNames[:10]\n",
    "    df = df[columnNames]\n",
    "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
    "    corrs = df.corr().values\n",
    "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
    "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
    "    plt.suptitle('Scatter and Density Plot')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPpJCdJKG_C7"
   },
   "source": [
    "## Distribution graphs (histogram/bar graph) of sampled columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoHdkrr5G704"
   },
   "outputs": [],
   "source": [
    "plotPerColumnDistribution(df_clean, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtsEF-D2Hbba"
   },
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXfRc0Q0HGd0"
   },
   "outputs": [],
   "source": [
    "plotCorrelationMatrix(df_clean, 53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfmtlGoOHr36"
   },
   "source": [
    "## Scatter and Density plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZcfiHHlHggt"
   },
   "outputs": [],
   "source": [
    "plotScatterMatrix(df_clean, 20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwfBkXBVH_zP"
   },
   "source": [
    "## Reduce the No of Columns by Creating New Meaningful Features\n",
    "Create Columns with Average of 6th & 7th Month Since it's a \"Good\" Phase and Keep the 8th month untouched as it's \"Action\" Phase, for now to see if it can give any additional insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wvph8VpfHwnf"
   },
   "outputs": [],
   "source": [
    "col_list = df_clean.filter(regex='_6|_7').columns.str[:-2]\n",
    "col_list.unique()\n",
    "\n",
    "for idx, col in enumerate(col_list.unique()):\n",
    "    print(col)\n",
    "    avg_col_name = \"avg_\"+col+\"_av67\"\n",
    "    col_6 = col+\"_6\"\n",
    "    col_7 = col+\"_7\"\n",
    "    df_clean[avg_col_name] = (df_clean[col_6]  + df_clean[col_7])/ 2\n",
    "\n",
    "\n",
    "\n",
    "# print (df_high_val_cust.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JNp9q1sN-69"
   },
   "outputs": [],
   "source": [
    "print (df_clean.shape)\n",
    "\n",
    "col_list = df_clean.filter(regex='_6|_7').columns\n",
    "\n",
    "df_clean.drop(col_list, axis=1, inplace=True)\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFhvOuLdQnjE"
   },
   "outputs": [],
   "source": [
    "#Conevrt AON in Months\n",
    "df_clean['aon_mon'] = df_clean['aon']/30\n",
    "df_clean.drop('aon', axis=1, inplace=True)\n",
    "df_clean['aon_mon'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jb0-Gp7yQ5Yr"
   },
   "source": [
    "## Churn Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVOXTavMQuO7"
   },
   "outputs": [],
   "source": [
    "#Churn Distribution\n",
    "ax = (df_clean['churn'].value_counts()*100.0 /len(df_clean)).plot.pie(autopct='%.1f%%', labels = ['No', 'Yes'],figsize =(5,5), fontsize = 12 )                                                                           \n",
    "\n",
    "ax.set_ylabel('Churn',fontsize = 12)\n",
    "ax.set_title('Churn Distribution', fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z57R-rGWRCU7"
   },
   "source": [
    "In our data, 54% of the customers do not churn. So, the data is slighly imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vagf6cGxRFKT"
   },
   "source": [
    "## Distribution graphs (histogram/bar graph) of sampled columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5lRHC11VQ9g-"
   },
   "outputs": [],
   "source": [
    "plotPerColumnDistribution(df_clean, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dT-_tdIMRMiM"
   },
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OhlKTvkoRHda"
   },
   "outputs": [],
   "source": [
    "plotCorrelationMatrix(df_clean, 53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_TBj2QaRTGN"
   },
   "source": [
    "## Scatter and density plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efuc8uTYRQQe"
   },
   "outputs": [],
   "source": [
    "plotScatterMatrix(df_clean, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxrAPdSXRXcf"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.distplot(df_clean['aon_mon'], hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})\n",
    "ax.set_ylabel('No of Customers')\n",
    "ax.set_xlabel('Tenure (months)')\n",
    "ax.set_title('Customers by their tenure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ofhg5u54ReOw"
   },
   "outputs": [],
   "source": [
    "tn_range = [0, 6, 12, 24, 60, 61]\n",
    "tn_label = [ '0-6 Months', '6-12 Months', '1-2 Yrs', '2-5 Yrs', '5 Yrs and above']\n",
    "df_clean['tenure_range'] = pd.cut(df_clean['aon_mon'], tn_range, labels=tn_label)\n",
    "df_clean['tenure_range'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZ1jtkiMRnaG"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "temp = pd.Series(data = 'tenure_range')\n",
    "fig, ax = plt.subplots()\n",
    "width = len(df_clean['tenure_range'].unique()) + 6 + 4*len(temp.unique())\n",
    "fig.set_size_inches(width , 7)\n",
    "\n",
    "total = float(len(df_clean.index))\n",
    "ax = sns.countplot(x=\"tenure_range\", data=df_clean, palette=\"Set2\", hue = \"churn\");\n",
    "for p in ax.patches:\n",
    "                ax.annotate('{:1.1f}%'.format((p.get_height()*100)/float(len(df_clean))), (p.get_x()+0.05, p.get_height()+20))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNkUadsMRx8J"
   },
   "source": [
    "## Correlation of \"Churn\" with other variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEQK0f-bRutu"
   },
   "outputs": [],
   "source": [
    "#Get Correlation of \"Churn\" with other variables:\n",
    "plt.figure(figsize=(20,10))\n",
    "df_clean.corr()['churn'].sort_values(ascending = False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZAMIjNyR5c6"
   },
   "source": [
    "Avg STD Outgoing Calls for Month 6 & 7, Outgoing calls in Roaming seems to be positively correlated with Churn while Avg Revenue, No Of Recharge for 8th Month seems negatively correlated.\n",
    "\n",
    "\n",
    "\n",
    "Lets look at the relation between total recharge in 8th Month Vs Average Revenue in 8th Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EtbFsOtwR2Bj"
   },
   "outputs": [],
   "source": [
    "df_clean[['total_rech_num_8', 'arpu_8']].plot.scatter(x = 'total_rech_num_8',\n",
    "                                                              y='arpu_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yo92p_0vTD-i"
   },
   "source": [
    "Lets look at the relation between Tenure And Revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6bqjbqoTSNy"
   },
   "source": [
    "## Churn vs Tenure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQSxqjqKR_hA"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x = df_clean.churn, y = df_clean.aon_mon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBXrjulOTb41"
   },
   "source": [
    "As we can see form the below plot, the customers who do not churn, they tend to stay for a longer tenure with the telecom company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xPEZ8G5Teib"
   },
   "source": [
    "## Churn Vs Volume based cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EwfANN6THEs"
   },
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(df_clean.avg_max_rech_amt_av67[(df_clean[\"churn\"] == 0)],\n",
    "                color=\"Red\", shade = True)\n",
    "ax = sns.kdeplot(df_clean.avg_max_rech_amt_av67[(df_clean[\"churn\"] == 1)],\n",
    "                ax =ax, color=\"Blue\", shade= True)\n",
    "ax.legend([\"Not Churn\",\"Churn\"])\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_xlabel('Volume based cost')\n",
    "ax.set_title('Distribution of Volume based cost by churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cuhknm2LTn2j"
   },
   "source": [
    "## Churn Vs Max Recharge Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Me2_Nh3jTj8N"
   },
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(df_clean.max_rech_amt_8[(df_clean[\"churn\"] == 0)],\n",
    "                color=\"Red\", shade = True)\n",
    "ax = sns.kdeplot(df_clean.max_rech_amt_8[(df_clean[\"churn\"] == 1)],\n",
    "                ax =ax, color=\"Blue\", shade= True)\n",
    "ax.legend([\"Not Churn\",\"Churn\"])\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_xlabel('Volume based cost')\n",
    "ax.set_title('Distribution of Max Recharge Amount by churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Pf2pvZUT2nP"
   },
   "source": [
    "* People Who Recharge with less Amount are more likely to Churn\n",
    "* There is no visible difference in Volume Based Cost & Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aykIyUW6T1eP"
   },
   "outputs": [],
   "source": [
    "#Lets Create New DF for Model Building\n",
    "\n",
    "df = df_clean[:].copy()\n",
    "\n",
    "#Dropping tenure_range since we have AON MONTH already and columns are highly coorelated\n",
    "df.drop('tenure_range', axis=1, inplace=True)\n",
    "\n",
    "#Since All The Values are realted to Price/ Cost/ Amount, Filling NaN with 0\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DupG_SECTtPV"
   },
   "outputs": [],
   "source": [
    "X = df.drop(['churn'], axis=1)\n",
    "y = df['churn']\n",
    "\n",
    "df.drop('churn', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1cdzUbNoZwJ"
   },
   "source": [
    "## Feature Scaling\n",
    " - Use StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxAOu8PUUBjT"
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TO25RKjogZ-"
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ib1TM3c5UDAa"
   },
   "outputs": [],
   "source": [
    "# Split in train & Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eei8_6AjUEQz"
   },
   "outputs": [],
   "source": [
    "print(\"X_train Shape : \", X_train.shape)\n",
    "print(\"X_test Shape : \", X_test.shape)\n",
    "\n",
    "y_train_imb = (y_train != 0).sum()/(y_train == 0).sum()\n",
    "y_test_imb = (y_test != 0).sum()/(y_test == 0).sum()\n",
    "print(\"Imbalance in Train Data : \", y_train_imb)\n",
    "print(\"Imbalance in Test Data : \", y_test_imb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SMOTE for Balancing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sPrxpAmaUGsb"
   },
   "outputs": [],
   "source": [
    "# Balancing DataSet\n",
    "sm = SMOTE(kind = \"regular\")\n",
    "X_tr,y_tr = sm.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6z7UuoH-UIW7"
   },
   "outputs": [],
   "source": [
    "print(\"X_tr Shape\", X_tr.shape)\n",
    "print(\"y_tr Shape\", y_tr.shape)\n",
    "\n",
    "imb = (y_tr != 0).sum()/(y_tr == 0).sum()\n",
    "print(\"Imbalance in Train Data : \",imb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3TkUlxxoIhP"
   },
   "source": [
    "## Modelling\n",
    "\n",
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UT55Zu33oQhU"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.svm = SVC(kernel = 'linear')\n",
    "lr.svm.fit(X_tr,y_tr)\n",
    "pred = lr.svm.predict(X_test)\n",
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYSBUOdz1yfR"
   },
   "source": [
    "## RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6vmxC-8qmYD"
   },
   "outputs": [],
   "source": [
    "# Reduce the features to 20 using Recursive Feature Eliminination\n",
    "rfe = RFE(lr,20)\n",
    "rfe = rfe.fit(X_tr,y_tr)\n",
    "df.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0Q_AQP_2G-h"
   },
   "outputs": [],
   "source": [
    "  X_rfe = pd.DataFrame(X_tr).iloc[:,rfe.support_]\n",
    "  y_rfe = y_tr\n",
    "  lr.fit(X_rfe,y_rfe)\n",
    "  X_test_rfe = pd.DataFrame(X_test).iloc[:,rfe.support_]\n",
    "  y_pred = lr.predict(X_test_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2dy8EjQC2uUK"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N6X01x6o4akQ"
   },
   "outputs": [],
   "source": [
    "print('Accuracy of Logistic Regression Model on test set is ',accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLzdBqwc6fZo"
   },
   "source": [
    "### Logistic Regression Model Summary\n",
    "\n",
    "- Model Accuracy is 82.5%\n",
    "- Confusion matrics clearly shows that the model has drawback in predicting high false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dh6PVbxq7GDD"
   },
   "source": [
    "## Principal Compound Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9UkLswz5Dym"
   },
   "outputs": [],
   "source": [
    "# Dimention Reduction using PCA\n",
    "pca = PCA(random_state=100)\n",
    "pca.fit(X_tr)\n",
    "\n",
    "X_tr_pca = pca.transform(X_tr)\n",
    "print(X_tr_pca.shape)\n",
    "\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCTSMRx28zKk"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MudWQ-Ty5KSd"
   },
   "outputs": [],
   "source": [
    "lr_pca = LogisticRegression()\n",
    "lr_pca.fit(X_tr_pca,y_tr)\n",
    "y_pred = lr_pca.predict(X_test_pca)\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cq3xI1Tu9UaN"
   },
   "outputs": [],
   "source": [
    "print('The Accuracy of Logistic Regression with PCA: ',accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DldKN10y-XUp"
   },
   "outputs": [],
   "source": [
    "#Making the screeplot - plotting the cumulative variance against the number of components\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vtp413y-fiF"
   },
   "outputs": [],
   "source": [
    "np.cumsum(np.round(pca.explained_variance_ratio_,decimals=4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MuMQQBPeBIyB"
   },
   "outputs": [],
   "source": [
    "pca_32 = PCA(n_components=32)\n",
    "\n",
    "df_tr_pca_32 = pca_32.fit_transform(X_tr)\n",
    "print(df_tr_pca_32.shape)\n",
    "\n",
    "df_test_pca_32 = pca_32.transform(X_test)\n",
    "print(df_test_pca_32.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "323L17VCCNVF"
   },
   "outputs": [],
   "source": [
    "lr_pca1 = LogisticRegression(C=1e9)\n",
    "lr_pca1.fit(df_tr_pca_32, y_tr)\n",
    "\n",
    "# Predicted probabilities\n",
    "y_pred32 = lr_pca1.predict(df_test_pca_32)\n",
    "\n",
    "# Converting y_pred to a dataframe which is an array\n",
    "df_y_pred = pd.DataFrame(y_pred32)\n",
    "\n",
    "# Print Confusion Matrix\n",
    "print(confusion_matrix(y_test,y_pred32))\n",
    "\n",
    "# Print Accuracy for Logistic Regression with PCA\n",
    "print('Logistic Regression accuracy with PCA ',accuracy_score(y_test,y_pred32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyQizXzmMQD1"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWkShqDrEqQK"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qnj3icEF2im"
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(kind='regular')\n",
    "X_tr,y_tr = sm.fit_sample(X_train,y_train)\n",
    "print(X_tr.shape)\n",
    "print(y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4R2iPb7-UtX"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    " \n",
    "lsvc = LinearSVC(C=0.001, penalty=\"l1\", dual=False).fit(X_tr, y_tr)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_lasso = model.transform(X_tr)\n",
    "pos = model.get_support(indices=True)\n",
    " ### Feature reduction using RFE\n",
    "print(X_lasso.shape)\n",
    "print(pos)\n",
    "#feature vector for decision tree\n",
    "lasso_features = list(df.columns[pos])\n",
    "print(\"Features for LASSO model buidling: \", lasso_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8iWfW8nMkIn"
   },
   "source": [
    "### Decision Tree With Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jS5IrxzNMOtc"
   },
   "outputs": [],
   "source": [
    "dt1 = DecisionTreeClassifier(max_depth=5)\n",
    "dt1.fit(X_lasso,y_tr)\n",
    "# Making predictions\n",
    "X_test = pd.DataFrame(data=X_test).iloc[:, pos]\n",
    "y_pred1 = dt1.predict(X_test)\n",
    "print(classification_report(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9timRfZzN0UU"
   },
   "outputs": [],
   "source": [
    "# Printing confusion matrix and accuracy\n",
    "print(confusion_matrix(y_test,y_pred1))\n",
    "print('Accuracy of Decision Tree :',accuracy_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kdKEJZdPju3"
   },
   "source": [
    "### Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEIcPtbaOb9o"
   },
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal max_depth\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameters = {'max_depth': range(1, 40)}\n",
    "\n",
    "# instantiate the model\n",
    "dtree = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                               random_state = 100)\n",
    "\n",
    "# fit tree on training data\n",
    "tree = GridSearchCV(dtree, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"accuracy\",return_train_score='warn')\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euUqMwVrm2j4"
   },
   "outputs": [],
   "source": [
    "# scores of GridSearch CV\n",
    "score = tree.cv_results_\n",
    "pd.DataFrame(score).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TJvraCqQntN"
   },
   "outputs": [],
   "source": [
    "# plotting accuracies with max_depth\n",
    "plt.figure()\n",
    "plt.plot(score[\"param_max_depth\"], \n",
    "         score[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(score[\"param_max_depth\"], \n",
    "         score[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GLNL9CDn4RX"
   },
   "source": [
    "According to the above plot, we can see the distribution of test and train accuracies for each value of max_depth within the range of 1 to 40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOmalcH5qdAw"
   },
   "source": [
    "# Tuning Min Samples Leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REa4vtkxTjMx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# parameter to build the model on\n",
    "parameters = {'min_samples_leaf':range(5,200,20)}\n",
    "\n",
    "# instantiate the model\n",
    "dtree = DecisionTreeClassifier(criterion='gini',random_state=100)\n",
    "\n",
    "# fit tree on training data\n",
    "tree = GridSearchCV(dtree,parameters,cv=n_folds,scoring='accuracy',return_train_score='warn')\n",
    "\n",
    "tree.fit(X_lasso,y_tr)\n",
    "\n",
    "score = tree.cv_results_\n",
    "pd.DataFrame(score).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m4vS5gH_ppy2"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(score['param_min_samples_leaf'],score['mean_train_score'],label='training accuracy')\n",
    "plt.plot(score['param_min_samples_leaf'],score['mean_test_score'],label='test accuracy')\n",
    "plt.xlabel(\"min sample leaf\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4aMii-htgZm"
   },
   "source": [
    "min_samples_leaf = 25 looks to be optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qdaOVZKuFTw"
   },
   "source": [
    "# Tuning min samples split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mv7YOFqXrfdQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nfolds = 5\n",
    "parameters = {'min_samples_split':range(5,200,20)}\n",
    "dtree = DecisionTreeClassifier(criterion='gini',random_state=100)\n",
    "tree = GridSearchCV(dtree,parameters,cv=nfolds,scoring='accuracy',return_train_score='warn')\n",
    "tree.fit(X_lasso,y_tr)\n",
    "\n",
    "score = tree.cv_results_\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(score['param_min_samples_split'],score['mean_train_score'],label='training accuracy')\n",
    "plt.plot(score['param_min_samples_split'],score['mean_test_score'],label='test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmYGrJUnyya0"
   },
   "source": [
    "# Grid Search for Optimal Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnWNOuDRu-vY"
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(25, 175, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "    'criterion': [\"entropy\", \"gini\"]\n",
    "}\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "# Instantiate the grid search model\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid = param_grid, \n",
    "                          cv = n_folds, verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_lasso, y_tr)\n",
    "# cv results\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results\n",
    "# printing the optimal accuracy score and hyperparameters\n",
    "print(\"Best Accuracy\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oTFgDJfF0Tsw"
   },
   "outputs": [],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlhfi6py0b6s"
   },
   "outputs": [],
   "source": [
    "# model with optimal hyperparameters\n",
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                  random_state = 100,\n",
    "                                  max_depth=5, \n",
    "                                  min_samples_leaf=25,\n",
    "                                  min_samples_split=50)\n",
    "clf_gini.fit(X_lasso, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSozKv0u0hVK"
   },
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "print ('Accuracy Score for Decision Tree Final Model :',clf_gini.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdSE9UBqsELq"
   },
   "outputs": [],
   "source": [
    "# plotting tree \n",
    "import pydotplus, graphviz\n",
    "from IPython.display import Image  \n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus, graphviz\n",
    "\n",
    "features = X_test.columns\n",
    "dot_data = StringIO()  \n",
    "export_graphviz(clf_gini, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wh7kTGKI0p0l"
   },
   "source": [
    "#### Summary - Decision Tress\n",
    "* Getting around 84% accuracy \n",
    "* Confusion matix shows lot of false positives still exist.\n",
    "* 31 Features were selected for Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4oTFAjzSWL3"
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "Random Forest with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XyvkNnuL0i7b"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_lasso,y_tr)\n",
    "\n",
    "# Make Predictions\n",
    "prediction_test = model_rf.predict(X_test)\n",
    "print('Random Forest Accuracy with default hyperparameters',metrics.accuracy_score(y_test,prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I55SqEBDUc1B"
   },
   "outputs": [],
   "source": [
    "## Printing the classification report viewing the accuracies\n",
    "print(classification_report(y_test,prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdibIUkYWKp4"
   },
   "outputs": [],
   "source": [
    "## Printing the Confusion Matrix\n",
    "print(confusion_matrix(y_test,prediction_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuMhdYg0W0Tb"
   },
   "source": [
    "### Hyper Parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8q5p81j5Wu0V"
   },
   "outputs": [],
   "source": [
    "# GridSearchCV to find the optimal n_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# specify no. of folds for k-fold cv\n",
    "n_folds = 5\n",
    "\n",
    "# parameter to build the model on\n",
    "parameters = {'max_depth':range(2,20,5)}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf,parameters,scoring='accuracy',return_train_score='warn')\n",
    "rf.fit(X_lasso,y_tr)\n",
    "\n",
    "score = rf.cv_results_\n",
    "\n",
    "# Show the plot to identify optimal value\n",
    "plt.figure()\n",
    "plt.plot(score['param_max_depth'],score['mean_train_score'],label='traning accuracy')\n",
    "plt.plot(score['param_max_depth'],score['mean_test_score'],label='test accuracy')\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel('accuracy_score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZQNASJAZIb7"
   },
   "source": [
    "## Tuning Min Sample Leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sn25cOXHYIix"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# specify number of folds for k fold cv\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameter = {'min_samples_leaf':range(50,400,10)}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf,parameter,cv = n_folds,scoring='accuracy',return_train_score='warn')\n",
    "rf.fit(X_lasso,y_tr)\n",
    "\n",
    "# Scores of Grid Search CV\n",
    "scores = rf.cv_results_\n",
    "\n",
    "# Plotting accuracies with min sample leaf\n",
    "plt.figure()\n",
    "plt.plot(scores['param_min_samples_leaf'],scores['mean_train_score'],label='train accuracy')\n",
    "plt.plot(scores['param_min_samples_leaf'],scores['mean_test_score'],label='test accuracy')\n",
    "plt.xlabel('min_samples_leaf')\n",
    "plt.ylabel('accuracy score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yat8M7_AidKO"
   },
   "source": [
    "## Tuning Min Samples Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Igk1R74xZwMQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# specify number of folds for k fold cv\n",
    "n_folds = 5\n",
    "\n",
    "# parameters to build the model on\n",
    "parameter = {'min_samples_split':range(100,500,25)}\n",
    "\n",
    "# instantiate the model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# fit tree on training data\n",
    "rf = GridSearchCV(rf,parameter,cv = n_folds,scoring='accuracy',return_train_score='warn')\n",
    "rf.fit(X_lasso,y_tr)\n",
    "\n",
    "# Scores of Grid Search CV\n",
    "scores = rf.cv_results_\n",
    "\n",
    "# Plotting accuracies with min sample leaf\n",
    "plt.figure()\n",
    "plt.plot(scores['param_min_samples_split'],scores['mean_train_score'],label='train accuracy')\n",
    "plt.plot(scores['param_min_samples_split'],scores['mean_test_score'],label='test accuracy')\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.ylabel('accuracy score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "icK9Az5zi8Zk"
   },
   "source": [
    "## Grid Search to Find Optimal Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cNJq4vmiyMJ"
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {'max_depth':[4,8,10],'min_samples_leaf':range(100,300,100),'min_samples_split':range(200,500,100),'n_estimators':[500,700],'max_features':[10,20,25]}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=rf,param_grid=param_grid,cv=3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WP1QqKl4kniw"
   },
   "outputs": [],
   "source": [
    "#Commenting as it takes long time\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_lasso, y_tr)\n",
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('Accuracy is',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6KWPUrEAk1Yh"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "model_rf = RandomForestClassifier(bootstrap=True,\n",
    "                                  max_depth=10,\n",
    "                                  min_samples_leaf=100, \n",
    "                                  min_samples_split=200,\n",
    "                                  n_estimators=1000 ,\n",
    "                                  oob_score = True, n_jobs = -1,\n",
    "                                  random_state =50,\n",
    "                                  max_features = 15,\n",
    "                                  max_leaf_nodes = 30)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "prediction_test = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5sk6gjilZeH"
   },
   "outputs": [],
   "source": [
    "# evaluation metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,prediction_test))\n",
    "print(confusion_matrix(y_test,prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyIWy-TOlbzx"
   },
   "outputs": [],
   "source": [
    "\n",
    "# accuracy score\n",
    "print ('Accuracy Score for Random Forest Final Model :',metrics.accuracy_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JrjQNaLjleQ7"
   },
   "outputs": [],
   "source": [
    "X = df\n",
    "# Scaling all the variables to a range of 0 to 1\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "features = X.columns.values\n",
    "X = pd.DataFrame(scaler.transform(X))\n",
    "X.columns = features\n",
    "\n",
    "importances = model_rf.feature_importances_\n",
    "weights = pd.Series(importances,\n",
    "                 index=X.columns.values)\n",
    "weights.sort_values()[-10:].plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZ8h5WU6nNS6"
   },
   "source": [
    "#### Observations\n",
    "- The results from random forest are very similar to that of the logistic regression \n",
    "- From random forest algorithm, Local Incoming for Month 8, Average Revenue Per Customer for Month 8 and Max Recharge Amount for Month 8 are the most important predictor variables to predict churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaIL-nLPnRzg"
   },
   "source": [
    "###  Final Recomendation : Telecom Churn\n",
    "* Std Outgoing Calls and Revenue Per Customer are strong indicators of Churn\n",
    "* People with less than 4 Yrs of Tenure are more likely to Churn\n",
    "* Behaviour of Volume Based Cost is not a strong indicator of Churn\n",
    "* Max Recharge Amount could be a good Churn Indicator\n",
    "* Random Forest is the best method to Predict Churn followed by SVM, other models too do a fair job\n",
    "* Behaviour in 8th Month can be the base of Churn Analysis\n",
    "* Local Incoming and Outgoing Calls for 8th Month and Average Revenue in 8th Month are strong indicators of Churn Behaviour"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Telecom_Study-v3.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
